{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data from csv\n",
    "\n",
    "pd.read_csv()\n",
    "\n",
    "fixed_df = pd.read_csv('../data/bikes.csv', sep=';', encoding='latin1', parse_dates=['Date'], dayfirst=True, index_col='Date')\n",
    "\n",
    "    parse_date : data column to be parsed \n",
    "    dayfirst : boolean, dateformat\n",
    "    index_col : column on which index is applied\n",
    "    sep : separator used in csv\n",
    "\n",
    "df = pd.read_csv(Location, names=['Names','Births'])\n",
    "\n",
    "    names : custom column names\n",
    "\n",
    "\n",
    "df2 = pd.read_csv(file_messy, delimiter=' ', header=3, comment='#')\n",
    "    \n",
    "    comment : remove comments\n",
    "    \n",
    "\n",
    "#### dataframe data types\n",
    "\n",
    "df.dtypes\n",
    "\n",
    "#### dataframe describe data \n",
    "\n",
    "    df.describe() \n",
    "        gives statistical data about numeric columns\n",
    "\n",
    "    df['col'].describe()                <- on categorical variable gives unique, count, freq, top    \n",
    "\n",
    "\n",
    "#### get first three rows\n",
    "\n",
    "    fixed_df[:3]\n",
    "\n",
    "#### selecting a column\n",
    "\n",
    "    fixed_df['Column_name']\n",
    "\n",
    "#### plotting column\n",
    "\n",
    "    fixed_df['Column_name'].plot()\n",
    "\n",
    "#### plot dataframe\n",
    "\n",
    "    fixed_df.plot(figsize=(15, 10))\n",
    "\n",
    "#### Make the graphs a bit prettier, and bigger\n",
    "\n",
    "    pd.set_option('display.mpl_style', 'default') \n",
    "    pd.set_option('display.line_width', 5000) \n",
    "    pd.set_option('display.max_columns', 60) \n",
    "\n",
    "#### plot histograms\n",
    "    gives distribution of data\n",
    "\n",
    "    df.hist()\n",
    "\n",
    "\n",
    "\n",
    "#### dataframe summary\n",
    "\n",
    "    fixed_df\n",
    "\n",
    "#### selecting a column\n",
    "\n",
    "    fixed_df['Column_name']\n",
    "\n",
    "#### selecting multiple column\n",
    "\n",
    "    fixed_df[['Column_1','Column_2']]\n",
    "\n",
    "#### value_counts()\n",
    "    number of data points for each uniques column value for given column name\n",
    "\n",
    "    complaints['Complaint Type'].value_counts()\n",
    "\n",
    "#### plot value count as bar graph\n",
    "\n",
    "    complaints['Complaint Type'].value_counts().plot(kind='bar')\n",
    "\n",
    "#### selecting data\n",
    "\n",
    "    noise_complaints = complaints[complaints['Complaint Type'] == \"Noise - Street/Sidewalk\"]\n",
    "\n",
    "    noise_complaints = complaints['boolean condition']\n",
    "\n",
    "#### pandas Series data types are numpy array\n",
    "\n",
    "    to convert pd.series to numpy array use .values\n",
    "\n",
    "    pd.Series([1,2,3]).values\n",
    "    array([1, 2, 3])\n",
    "\n",
    "    pd.Series(np.array([1, 2, 3]))\n",
    "\n",
    "\n",
    "#### change datatype \n",
    "\n",
    "    .astype('data_type')\n",
    "\n",
    "#### adding a column to dataframe\n",
    "\n",
    "    berri_bikes['column_name'] = berri_bikes.index.weekday\n",
    "\n",
    "#### dataframe groupby() method \n",
    "    it is similar to sql group by\n",
    "\n",
    "    weekday_count = berri_bikes.groupby('column_name').aggregate(sum)\n",
    "\n",
    "    adding index to it\n",
    "\n",
    "    weekday_counts.index = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "\n",
    "### Getting data via network\n",
    "\n",
    "    url_template = \"http://climate.weather.gc.ca/climateData/bulkdata_e.html?format=csv&stationID=5415&Year={year}&Month={month}&timeframe=1&submit=Download+Data\"\n",
    "\n",
    "    url = url_template.format(month=3, year=2012)\n",
    "\n",
    "    the same read_csv function can be used to get data from network\n",
    "\n",
    "    weather_mar2012 = pd.read_csv(url, skiprows=16, index_col='Date/Time', parse_dates=True, encoding='latin1')\n",
    "\n",
    "#### drop columns with null values\n",
    "\n",
    "    weather_mar2012.dropna(axis=1, how='any')\n",
    "    axis=1 : column\n",
    "\n",
    "#### drop columns \n",
    "\n",
    "    weather_mar2012 = weather_mar2012.drop(['Year', 'Month', 'Day', 'Time', 'Data Quality'], axis=1)\n",
    "\n",
    "#### concat dataframes\n",
    "\n",
    "    data_by_month = [download_weather_month(2012, i) for i in range(1, 13)]\n",
    "    weather_2012 = pd.concat(data_by_month)\n",
    "\n",
    "### String operation\n",
    "\n",
    "    is_snowing = weather_description.str.contains('Snow')\n",
    "\n",
    "#### median of each operation\n",
    "\n",
    "    weather_2012['Temp (C)'].resample('M', how=np.median).plot(kind='bar')\n",
    "\n",
    "#### plotting two bar graphs\n",
    "\n",
    "    stats = pd.concat([temperature, snowiness], axis=1)\n",
    "\n",
    "    stats.plot(kind='bar', subplots=True, figsize=(15, 10))\n",
    "\n",
    "#### to datetime\n",
    "\n",
    "    pd.to_datetime\n",
    "\n",
    "\n",
    "#### sort dataframe values\n",
    "\n",
    "    df.sort_values(['columns'], ascending=True)\n",
    "\n",
    "#### max\n",
    "\n",
    "    df['colums'].max()\n",
    "\n",
    "#### unique\n",
    "\n",
    "    df['colums'].unique()\n",
    "\n",
    "\n",
    "### save data to csv\n",
    "\n",
    "    df.to_csv('births1880.csv',index=False,header=False)\n",
    "\n",
    "\n",
    "### apply function to dataframe column\n",
    "\n",
    "    df['State'] = df.State.apply(lambda x: x.upper())\n",
    "\n",
    "\n",
    "## read data from sql\n",
    "\n",
    "    import pandas.io.sql\n",
    "    import pyodbc\n",
    "\n",
    "    server = 'DAVID-THINK'\n",
    "    db = 'BizIntel'\n",
    "\n",
    "    conn = pyodbc.connect('DRIVER={SQL Server};SERVER=' + DB['servername'] + ';DATABASE=' + DB['database'] + ';Trusted_Connection=yes')\n",
    "\n",
    "    sql = \"\"\"\n",
    "\n",
    "    SELECT top 5 *\n",
    "    FROM data\n",
    "\n",
    "    \"\"\"\n",
    "    df = pandas.io.sql.read_sql(sql, conn)\n",
    "    df.head()\n",
    "\n",
    "\n",
    "#### method 2\n",
    "\n",
    "\n",
    "    from sqlalchemy import create_engine\n",
    "\n",
    "    ServerName = \"DAVID-THINK\"\n",
    "    Database = \"BizIntel\"\n",
    "    Driver = \"driver=SQL Server Native Client 11.0\"\n",
    "\n",
    "    engine = create_engine('mssql+pyodbc://' + ServerName + '/' + Database + \"?\" + Driver)\n",
    "\n",
    "    df = pd.read_sql_query(\"SELECT top 5 * FROM data\", engine)\n",
    "\n",
    "\n",
    "### dataframe to json\n",
    "\n",
    "    df.to_json('Lesson10.json')\n",
    "\n",
    "#### read json into python dataframe\n",
    "\n",
    "    df2 = pd.read_json(jsonloc)\n",
    "\n",
    "\n",
    "\n",
    "#### drop missing values\n",
    "\n",
    "    df2.dropna()\n",
    "\n",
    "#### fill missing values\n",
    "\n",
    "    df2[column].fillna('value')\n",
    "\n",
    "\n",
    "#### apply\n",
    "\n",
    "    applies function along any axis of dataframe\n",
    "\n",
    "    df['col'].apply(np.sqrt)\n",
    "\n",
    "#### applymap\n",
    "\n",
    "    applies function to entire dataframe\n",
    "\n",
    "    df.applymap(np.sqrt)\n",
    "\n",
    "## dataframe stats\n",
    "\n",
    "    describe() : describes all the column\n",
    "    cov() : covariance between suitable column\n",
    "    corr() : correation between columns\n",
    "\n",
    "#### dataframe joins\n",
    "\n",
    "    pd.merge(df,other,on='str_col',how='left')\n",
    "\n",
    "\n",
    "## create categorical indexing in dataframe\n",
    "\n",
    "    incomeranges = pd.cut(df['applicant_income_000s'], 14)\n",
    "\n",
    "    pd.value_counts(incomeranges)\n",
    "\n",
    "\n",
    "### dataframe indexing\n",
    "\n",
    "\n",
    "## scatter matrix\n",
    "\n",
    "    scatter plot with x and y as different columns\n",
    "\n",
    "    pd.scatter_matrix(df, diagonal='kde', figsize=(10, 10));\n",
    "\n",
    "\n",
    "## correlation map\n",
    "\n",
    "    corr = df.corr()\n",
    "    plt.imshow(corr, cmap='hot', interpolation='none')\n",
    "    plt.colorbar()\n",
    "    plt.xticks(range(len(corr)), corr.columns)\n",
    "    plt.yticks(range(len(corr)), corr.columns);\n",
    "\n",
    "## scatter plot\n",
    "\n",
    "    plt.scatter(rets.mean(), rets.std())\n",
    "    plt.xlabel('Expected returns')\n",
    "    plt.ylabel('Risk')\n",
    "\n",
    "# dataframe operations\n",
    "\n",
    "        Operation                            Syntax\t        Result\n",
    "        Select column\t                     df[col]\t    Series\n",
    "        Select row by label\t                df.loc[label]\tSeries\n",
    "        Select row by integer location\t    df.iloc[loc]\tSeries\n",
    "        Slice rows\t                         df[5:10]\t   DataFrame\n",
    "        Select rows by boolean vector\t    df[bool_vec]   DataFrame\n",
    "\n",
    "\n",
    "\n",
    "        Function\tDescription\n",
    "        count\tNumber of non-null observations\n",
    "        sum\t    Sum of values\n",
    "        mean\tMean of values\n",
    "        mad\t    Mean absolute deviation\n",
    "        median\tArithmetic median of values\n",
    "        min\t    Minimum\n",
    "        max\t    Maximum\n",
    "        mode\tMode\n",
    "        abs\t    Absolute Value\n",
    "        prod\tProduct of values\n",
    "        std\t    Bessel-corrected sample standard deviation\n",
    "        var\t    Unbiased variance\n",
    "        sem\t    Standard error of the mean\n",
    "        skew\tSample skewness (3rd moment)\n",
    "        kurt\tSample kurtosis (4th moment)\n",
    "        quantile\tSample quantile (value at %)\n",
    "        cumsum\tCumulative sum\n",
    "        cumprod\tCumulative product\n",
    "        cummax\tCumulative maximum\n",
    "        cummin\tCumulative minimum\n",
    "\n",
    "\n",
    "### dataframe indexes as date\n",
    "\n",
    "    index = pd.date_range('1/1/2000', periods=8)\n",
    "\n",
    "    df = pd.DataFrame(np.random.randn(8, 3), index=index, columns=list('ABC'))\n",
    "\n",
    "### to get raw data in the form of np array use df.values\n",
    "\n",
    "### aggregation functions\n",
    "\n",
    "     tsdf.agg(['sum', 'mean'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Time Series in pandas\n",
    "\n",
    "    df1 = pd.read_csv(filename)\n",
    "\n",
    "    df2 = pd.read_csv(filename, parse_dates=['Date'])\n",
    "        parse_dates -> parse date entries, argument = column names\n",
    "    \n",
    "    df3 = pd.read_csv(filename, index_col='Date', parse_dates=True)\n",
    "        parse_dates=True -> enable date parsing\n",
    "        index_col -> sets the index column of dataframe\n",
    "        \n",
    "        \n",
    "    Use format in to_datetime : Prepare a format string: time_format\n",
    "    \n",
    "    time_format = '%Y-%m-%d %H:%M'\n",
    "    my_datetimes = pd.to_datetime(date_list, format=time_format)    \n",
    "    \n",
    "### parsed dates supports partial indexing on date index column\n",
    "    df.loc['2015']\n",
    "    df.loc['2015-02']\n",
    "    df.loc['2015-02-02']\n",
    "    \n",
    "### date range\n",
    "    df.loc['2015' : '2016']\n",
    "    \n",
    "    \n",
    "### reindexing the column\n",
    "\n",
    "    df.reindex(df0.index, method = 'ffill')               -> ffill forward fill, fills NA values\n",
    "    \n",
    "    \n",
    "## Resampling\n",
    "\n",
    "    allows to apply statistical methods on time series, sum() mean() median() std()\n",
    "\n",
    "### downsampling\n",
    "\n",
    "    reduce datetime rows\n",
    "    going from daily to weekly\n",
    "\n",
    "### upsampling\n",
    "\n",
    "    increase datetime rows\n",
    "    going from daily to hourly\n",
    "    \n",
    "    \n",
    "    df.resample('D').mean()                    -> resample by day and calculate mean. where index column is datetime\n",
    "    \n",
    "    \n",
    "    resampling strings\n",
    "    \n",
    "    H -> hour\n",
    "    D -> day\n",
    "    B -> business day\n",
    "    W -> week\n",
    "    M -> month\n",
    "    Q -> quarter\n",
    "    A -> year\n",
    "    \n",
    "    resampling strings can be multiplied\n",
    "    \n",
    "    df.resample('2W').mean()                    -> gives bi-weekly average\n",
    "    \n",
    "    \n",
    "## rolling()\n",
    "\n",
    "    A moving average is commonly used with time series data to smooth out short-term fluctuations and highlight longer-term trends or cycles.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String transformation .str\n",
    "\n",
    "    df['col'].str.<method>()\n",
    "    .contains()\n",
    "    .upper()\n",
    "    .lower()\n",
    "    \n",
    "## Date transformation .dt    \n",
    "\n",
    "    df.dt.hour\n",
    "    df.dt.tz_localize('US/Central')\n",
    "    df.dt.tz_convert('US/Eastern')\n",
    "\n",
    "\n",
    "#### For decade data of population, get yearly smooth time series\n",
    "    \n",
    "    population.resample('A').first().interpolate('linear')\n",
    "    \n",
    "    \n",
    "## Visualizing pandas time series\n",
    "\n",
    "    style the .plot method\n",
    "    \n",
    "    .plot(style='')\n",
    "    \n",
    "    1. color      (k : black)\n",
    "    2. marker     (. : dot)\n",
    "    3. line-type  (- : solid)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing in python\n",
    "\n",
    "\n",
    "## loc vs iloc\n",
    "\n",
    "    loc : access using labels\n",
    "    iloc : access using index\n",
    "    \n",
    "    \n",
    "    df.loc[row, columns]\n",
    "    df.iloc[row_index, col_index]\n",
    "    \n",
    "\n",
    "    df['col']       -> series \n",
    "    df[['col']]     -> dataframe\n",
    "    \n",
    "####  in reverse order. To do this for hypothetical row labels 'a' and 'b', you could use a stepsize of -1 like so: df.loc['b':'a':-1].\n",
    "\n",
    "\n",
    "## filtering\n",
    "\n",
    "    df.col > 10\n",
    "    \n",
    "#### selecting via dataframe\n",
    "\n",
    "    df[df.col > 10]\n",
    "    \n",
    "### all vs any\n",
    "\n",
    "#### select columns with all non zero values\n",
    "\n",
    "    df.loc[:,df.all()]\n",
    "\n",
    "#### select columns with any non zero values\n",
    "\n",
    "    df.loc[:,df.any()]\n",
    "\n",
    "### chaining with isnull to filter NaN values\n",
    "\n",
    "#### select columns with all non NaN values\n",
    "\n",
    "    df.loc[:,df.notnull().all()]\n",
    "\n",
    "#### select columns with any non NaN values\n",
    "\n",
    "    df.loc[:,df.isnull().any()]\n",
    "    \n",
    "### remove data NaN's\n",
    "\n",
    "    df.dropna(how='any')\n",
    "\n",
    "    titanic.dropna(thresh=1000, axis='columns')           -> thresh = x; drops column if column has more than x NaN \n",
    "    \n",
    "## .apply() .map()    \n",
    "    \n",
    "    pd.series = df.col.apply(function)\n",
    "    pd.series = df.col.map(dict)\n",
    "    \n",
    "    these function performs looping over dataframe which reduces performance instead use numpy functions\n",
    "    \n",
    "    \n",
    "## dataframe index \n",
    "    \n",
    "    List of labels\n",
    "    df.index \n",
    "    \n",
    "    assignment\n",
    "    \n",
    "    df.index = pd.Series([1,2,3])\n",
    "    \n",
    "    \n",
    "### Hierarchical index - two or more columns as index\n",
    "\n",
    "    df = df.set_index(['col1', 'col2'])\n",
    "    \n",
    "    df.sort_index()                     -> to sort df w.r.t index\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## pivoting dataframe\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
